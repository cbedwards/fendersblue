---
title: "Online Resource 1"
author: "Collin Edwards"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE)
seed=123
```

R code for model fitting and calculations for __Changes in phenology and abundance of an at-risk butterfly__.

__Authors:__ Rachael E. Bonoan, Elizabeth Crone, Collin B. Edwards, Cheryl B. Schultz

## Libraries

```{r, cache=FALSE}
require(lubridate)
library(viridis) #package for colors
library(here) #package for handling file paths easily
library(beepr) #package for making sound to let me know things are done. Not important for document compile, but useful for running time-consuming loops or tasks.
require(lme4)
require(openxlsx)
require(prodlim)
require(msm)
require(tidyverse)
require(ggplot2)
require(MASS)
library(knitr)
library(kableExtra)
```


## Function definitions

```{r coefs_unscaled}
## Function for reversing scaling of coefficient estimates
coefs_unscaled = function(fixefs, mean, sd){
  #fixefs: the 3 coefficients of the regression fitted to scaled predictor
  #  in either matrix form or vector form
  #mean: mean of original unscaled predictor
  #sd: sd of original unscaled predictor
  ## the equations below can be obtained by doing the algebra
  ## based on the transformation of scaling
  if(!is.matrix(fixefs)){
    fixefs=matrix(fixefs, nrow=1) 
  }
  fixefs[,1] <- fixefs[,1] - fixefs[,2] * mean / sd + fixefs[,3] * mean^2 / sd^2
  fixefs[,2] <- fixefs[,2] / sd - 2 * fixefs[,3] * mean / sd^2
  fixefs[,3] <- fixefs[,3] / sd^2
  return(fixefs)
}
```

```{r pheno_calc}
## Function to calculate phenology metrics from vector of UNSCALED coefficients:
# mu = day of peak activity
# sd = sd of activity peak
# fp = "flight period" - # of days from 0.1 to 0.9 activity period
# fst = day of 0.1 activity, metric for first day
# N = abundance estimate
# h = height of peak
# badfit -> TRUE if exponential fit (quadratic term positive and/or 
pheno_calc=function(coefs #vector or matrix of coefficients, ordered from b0 to b2 (if matrix, columns ordered that way)
){
  #convert vector to matrix if needed
  if(!is.matrix(coefs)){
    coefs=matrix(coefs, nrow=1) 
  }
  res=data.frame(mu=numeric(nrow(coefs)),
                 sd=numeric(nrow(coefs)),
                 fp=numeric(nrow(coefs)),
                 fst=numeric(nrow(coefs)),
                 N=numeric(nrow(coefs)),
                 h=numeric(nrow(coefs)),
                 badfit = (coefs[,2] < 0) | (coefs[,3]>0)
  )
  rownames(res)=rownames(coefs)
  res$mu=coefs[,2]/(-2*coefs[,3])
  res$sd=sqrt(1/(-2*coefs[,3]))
  res$fp=qnorm(.9, mean=res$mu, sd=res$sd)-qnorm(.1, mean=res$mu, sd=res$sd)
  res$fst=qnorm(.1, mean=res$mu, sd=res$sd)
  res$N=sqrt(pi)/sqrt(-coefs[,3])*exp(coefs[,1]+coefs[,2]^2/(-4*coefs[,3]))
  res$h=exp(coefs[,1] + coefs[,2]*res$mu + coefs[,3]*res$mu^2)
  res$ofs=-sqrt(coefs[,2]^2 - 4 * coefs[,1] * coefs[,3])/coefs[,3]
  res$ofs[is.na(res$ofs)]=0
  return(res)
}
```

```{r dat_fit}
dat_fit=function(dat, #data with whatever zeroes added
                 doymean, #mean of the doy before zeroes added
                 doysd, #sd of the doy before zeroes added
                 scenario, #name of scenario. Number of zeroes, monthly, base, whatever. Character class.
                 yrs.good=NULL #if a vector, identifies the years to use (e.g. if a year of data is problematic, generally from having a single observation). If NULL, use all years 
){
  ## this function takes a data frame of data, and calculates metrics for each year
  #scale doy wisely
  dat$doy.sc <- (dat$doy-doymean)/doysd
  ## grabbing years with at least 3 positive counts
  yrcount=table(dat$Year, dat$Est_Count>0)
  if(is.null(yrs.good)){
    yrs.good=as.numeric(rownames(yrcount)[yrcount[,2]>0])
  }
  #number of good years
  dat = dat[dat$Year %in% yrs.good, ]
  out = glmer(Est_Count ~ doy.sc + I(doy.sc^2) + (doy.sc + I(doy.sc^2) | Year), family = poisson, data = dat,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
  coefs=coefficients(out)[[1]]
  coefs = as.matrix(coefs)
  # print("fit done")
  coefs.unsc=coefs_unscaled(fixefs=coefs,
                            mean=doymean,
                            sd=doysd)
  # print("unscaling done")
  cur.pheno=pheno_calc(coefs.unsc)
  # print("pheno calc done")
 
    ## Now we create a data frame to store our results
  res=data.frame(scenario=rep(scenario,nrow(coefs)),
                 year=as.numeric(rownames(coefs.unsc)),
                 b0=coefs.unsc[,1],
                 b1=coefs.unsc[,2],
                 b2=coefs.unsc[,3],
                 mu=cur.pheno$mu, #day of peak activity
                 fp=cur.pheno$fp, #flight period
                 N=cur.pheno$N, #abundance index
                 tenperc=cur.pheno$fst, #day of 10% observation
                 h=cur.pheno$h, #peak activity
                 ofs=cur.pheno$ofs, #observable flight period
                 badfit=cur.pheno$badfit #was the best-fitting model implausible?
  )
  
 
  yrs=as.numeric(rownames(coefs.unsc))
 
  ## store changes in metrics through time
  slope.sum=data.frame(mu=NA,
                       fp=NA,
                       ofs=NA,
                       logN=NA
  )
  # if there are no bad fits
  if(!any(res$badfit)){
    out.mu=lm(cur.pheno$mu ~ yrs)
    out.fp=lm(cur.pheno$fp ~ yrs)
    out.ofs=lm(cur.pheno$ofs ~ yrs)
    out.logN=lm(log(cur.pheno$N) ~ yrs)
    slope.sum$mu=coefficients(out.mu)[2]
    slope.sum$fp=coefficients(out.fp)[2]
    slope.sum$ofs=coefficients(out.ofs)[2]
    slope.sum$logN=coefficients(out.logN)[2]
    
  }
  
  ## store mean vals across all years, with 95% CI
  # add summary info to res.sum
  names(slope.sum)=sprintf("%s.%s", "slope", names(slope.sum))
  
  return(list(df=res,
              summary=data.frame(
                scenario=scenario,
                slope.sum
              )))
}
```

```{r plot_fits}
#######################
## function to plot data points. 
#  If given a fits data frame (as from dat_fit), will plot best-fitting furve
#  If given a "sim" list, will add +/- 1 SE% CIs.
plot_fits=function(dat, #data to plot (after filtering to site, removing extraneous years if needed)
                   fits=NULL, #data frame of yearly fits
                   sim=NULL, #list of bootstrapped simulations
                   xlim=c(100,200) #range to plot
){
  par(mfrow=c(2,2), mar=c(6,6,2,2))
  for(cur.year in unique(dat$Year)){
    flag.fits=FALSE #should we plot the best-fitting curve?
    ## grab best-fitting curve, if it exists
    if(!is.null(fits)){
      fits.cur=fits[fits$year==cur.year,]
      if(nrow(fits.cur)==1){
        flag.fits=TRUE
      }
    }
    #put together best-fitting curve, if it exists
    x.plot=seq(xlim[1],xlim[2],by=.1)
    y.plot=NULL
    if(flag.fits){
      y.plot= exp(fits.cur$b0 + fits.cur$b1 * x.plot +fits.cur$b2 * x.plot^2)
    }
    #put together CIs, if they exist
    ci.low=ci.high=NULL
    flag.ci=FALSE
    if(!is.null(sim)){
      sim.cur=sim[[as.character(cur.year)]]
      if(!is.null(sim.cur)){
        flag.ci=TRUE
      }
    }
    if(flag.ci){
      pred.grid=matrix(-99,
                       nrow=nsample,
                       ncol=length(x.plot))
      for(i.boot in 1:nsample){
        pred.grid[i.boot,]=exp(sim.cur$b0[i.boot] +
                                 sim.cur$b1[i.boot]*x.plot +
                                 sim.cur$b2[i.boot]*x.plot^2)
        
      }
      # ci.high=apply(pred.grid, 2, function(x) quantile(x, probs=.975))
      ci.high=apply(pred.grid, 2, function(x) quantile(x, probs=pnorm(c(1))))
      # ci.low=apply(pred.grid, 2, function(x) quantile(x, probs=.025))  
      ci.low=apply(pred.grid, 2, function(x) quantile(x, probs=pnorm(c(-1))))  
    }
    
    plot(x=dat$doy[dat$Year==cur.year],
         y=dat$Est_Count[dat$Year==cur.year],
         ylim=range(dat$Est_Count[dat$Year==cur.year],y.plot,ci.low, ci.high),
         xlim=xlim,
         pch=18,
         cex=1.5,
         xlab="DOY",
         ylab="Count",
         cex.lab=1.8,
         cex.axis=1.6,
         main=cur.year,
         cex.main=1.6
    )
    if(flag.fits){
      points(x.plot,y.plot,type='l', col='blue')
    }
    if(flag.ci){
      points(x.plot,ci.low,type='l', col='darkgrey', lty=2)
      points(x.plot,ci.high,type='l', col='darkgrey', lty=2)
    }
  }
}


```

```{r plot_comp}
plot_comp=function(dat, #data to plot (after filtering to site, removing extraneous years if needed)
                   fits.full,
                   fits.indiv,
                   fits.site, #site effect of full model
                   fits.year, #year effect of full model
                   site,
                   xlim=c(100,200) #range to plot
){
  par(mfrow=c(2,2), mar=c(6,6,2,2))
  ind=1
  for(cur.year in unique(dat$Year)){
    # print(cur.year)
    flag.fits=FALSE #should we plot the best-fitting curve?
    ## grab best-fitting curve, if it exists
    fits.ind.cur=fits.indiv[fits.indiv$year==cur.year,]
    fits.fu.cur=fits.full[rownames(fits.full)==cur.year,]
    fits.site.cur=fits.site #fit of site-only for full model
    fits.year.cur=fits.year[rownames(fits.year)==cur.year,] #fit of year only for full model
    # print(fits.fu.cur)
    if(nrow(fits.ind.cur)==1){
      flag.fits=TRUE
    }
    #put together best-fitting curve, if it exists
    x.plot=seq(xlim[1],xlim[2],by=.1)
    y.ind.plot=y.fu.plot=y.site.plot=y.year.plot=NULL
    if(flag.fits){
      y.ind.plot= exp(fits.ind.cur$b0 + fits.ind.cur$b1 * x.plot +fits.ind.cur$b2 * x.plot^2)
      y.fu.plot= exp(fits.fu.cur[[1]] + fits.fu.cur[[2]] * x.plot +fits.fu.cur[[3]] * x.plot^2)
      y.site.plot= exp(fits.site.cur[[1]] + fits.site.cur[[2]] * x.plot +fits.site.cur[[3]] * x.plot^2)
      y.year.plot= exp(fits.year.cur[[1]] + fits.year.cur[[2]] * x.plot +fits.year.cur[[3]] * x.plot^2)
    }
    plot(x=dat$doy[dat$Year==cur.year],
         y=dat$Est_Count[dat$Year==cur.year],
         ylim=range(dat$Est_Count[dat$Year==cur.year],y.ind.plot, y.fu.plot, y.site.plot, y.year.plot),
         xlim=xlim,
         pch=18,
         cex=1.5,
         xlab="DOY",
         ylab="Count",
         cex.lab=1.8,
         cex.axis=1.6,
         main=paste(site, cur.year),
         cex.main=1.6
    )
    if(flag.fits){
      points(x.plot,y.ind.plot,type='l', col='black')
      points(x.plot,y.fu.plot,type='l', col='purple',lty=1)
      points(x.plot,y.site.plot,type='l', col='cornflowerblue',lty=3)
      points(x.plot,y.year.plot,type='l', col='indianred',lty=2)
    }
    if(ind %% 4 ==1){
      # print("legending")
      legend("topleft",
             legend=c("site model fit",
                      "full model: full fit",
                      "full model: avg site fit",
                      "full model: avg year fit"),
             fill=c("black",
                    "purple",
                    "cornflowerblue",
                    "indianred"))
    }
    ind=ind+1
  }
}

plot_comp2=function(dat, #data to plot (after filtering to site, removing extraneous years if needed)
                    fits.full,
                    fits.indiv,
                    # fits.site, #site effect of full model
                    # fits.year, #year effect of full model
                    site,
                    xlim=c(100,200) #range to plot
){
  par(mfrow=c(2,2), mar=c(6,6,2,2))
  ind=1
  for(cur.year in unique(dat$Year)){
    # print(cur.year)
    flag.fits=FALSE #should we plot the best-fitting curve?
    ## grab best-fitting curve, if it exists
    fits.ind.cur=fits.indiv[fits.indiv$year==cur.year,]
    fits.fu.cur=fits.full[fits.full$year==cur.year,]
    # print(fits.fu.cur)
    if(nrow(fits.ind.cur)==1){
      flag.fits=TRUE
    }
    #put together best-fitting curve, if it exists
    x.plot=seq(xlim[1],xlim[2],by=.1)
    y.ind.plot=y.fu.plot=y.site.plot=y.year.plot=NULL
    if(flag.fits){
      y.ind.plot= exp(fits.ind.cur$b0 + fits.ind.cur$b1 * x.plot +fits.ind.cur$b2 * x.plot^2)
      y.fu.plot= exp(fits.fu.cur$b0 + fits.fu.cur$b1 * x.plot +fits.fu.cur$b2 * x.plot^2)
    }
    plot(x=dat$doy[dat$Year==cur.year],
         y=dat$Est_Count[dat$Year==cur.year],
         ylim=range(dat$Est_Count[dat$Year==cur.year],y.ind.plot, y.fu.plot),
         xlim=xlim,
         pch=18,
         cex=1.5,
         xlab="DOY",
         ylab="Count",
         cex.lab=1.8,
         cex.axis=1.6,
         main=paste(site, cur.year),
         cex.main=1.6
    )
    if(flag.fits){
      points(x.plot,y.ind.plot,type='l', col='black')
      points(x.plot,y.fu.plot,type='l', col='purple',lty=2)
    }
    if(ind %% 4 ==1){
      # print("legending")
      legend("topleft",
             legend=c("site model fit",
                      "full model: full fit",
                      "full model: avg site fit",
                      "full model: avg year fit"),
             fill=c("black",
                    "purple",
                    "cornflowerblue",
                    "indianred"))
    }
    ind=ind+1
  }
}




```

# Data analysis

Note that we are taking the approach of adding 0 count observations to the middle of each month that we know they can't plausibly be active: Jan, feb, mar, aug, sept, oct, nov, and dec. These correspond to days 15, 46, 74, 227, 258, 288, 319, 349

## Quick note on OFS (Observable Flight Season)

We are adding in a novel metric, which is the duration of time in which >1 individual is predicted by the Guassian curve. Our gaussian curve is $e^{\beta_0+\beta_1x+\beta_2x^2}$, which equals 1 when the quadratic equation is zero ($\beta_0+\beta_1x+\beta_2x^2=0$). This is a classic problem solved by the quadratic formula, 

\[x=\frac{-b \pm \sqrt{b^2-4ac}}{2a}\]

We can simplify our life by noting that we are interested in the difference between the two solutions, which is simply

\[OFS=|\frac{2\sqrt{b^2-4ac}}{2a}|\]

For appropriate translation of $a$, $b$, and $c$ into our beta terms ($a=\beta_2$,$b=\beta_1$, $c=\beta_0$), and canceling the 2s, we find

\[OFS=\frac{-\sqrt{\beta_1^2-4\beta_2\beta_0}}{\beta_2}\]

(the negative sign is because $\beta_2$ will always be negative, and we want the absolute value, which is positive.

## Reading in the data, formatting

Notes: 
-  here I am continuing the previous filter of removing years 1996-1999 in FR_GreenOaks

```{r}
nsample=1000 #number of bootstraps to aim for
dat=raw=read.csv(here("1_raw_data","SERDP Phenology - Fenders Blue - 1993-2019 20191203.csv"))

## We are filtering out FR_GreenOaks
yrs.remove=c(1996, 1997, 1998, 1999)
dat= dat[!(dat$Site=="FR_GreenOaks" & dat$Year %in% yrs.remove),]

# convert date from factor to date object
dat$Date=mdy(dat$Date)
# adding DOY variable
dat$doy=yday(dat$Date)

# convert site to character
dat$Site=as.character(dat$Site)

#There are a few fractional counts:
sum((dat$Est_Count %% 1) != 0)
# we will round those.
dat$Est_Count=round(dat$Est_Count)

## save formatted data
saveRDS(dat,here("2_data_wrangling","cleandat.RDS"))
```

## Analyzing by site {.tabset}

**This preliminary analysis was carried out to (a) test our general intuition, and (b) identify problem sites and site-years that need to be excluded from the full analysis (below)**

This eventually creates a total of 2 results objects. `res.sum` has one row per site, and stores summaries of the site (means across years of peak abundance, flight period, and N; slopes through time of peak abundance, flight period, and log(N); %implausible values).  `res.df` is a data frame with one entry per year per site, summarizing information about that year. 

Flight period is now calculated as the time between the day of 0.1 activity, and day of 0.9 activity.

Note: we're going to try to fit each site. Full site list is `r unique(dat$Site)`.


```{r}
## objects to store results:
res.df=NULL # data frame with row per year
res.sum=NULL # Data frame with summaries of sites, one row per site.
dat.sim=list() # will become list of lists of bootstrapped coefficients
dat.sim.sum=list() # will become list of lists of bootstrapped coefficients
nsample=1000 #number of bootstrapped curves to generate

sites.use = c("Fir_Butte",
              "FR_Eaton",
              "FR_GreenOaks",
              "FR_RoyalAve",
              "FR_Shore",
              "FR_Spires",
              "WC_Bailey",
              "WC_FirGrove",
              "WC_Hayfield",
              "WC_Main",
              "WC_North"
)

good.df=NULL #store sites and years selected - for use in simultaneous fitting
fits.indiv=list()
```


### WC Main{.tabset}



```{r}
cur.site="WC_Main"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```



#### yearly plots

```{r}
dat.use=dat.use.base
plot_fits(dat.use)
table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)


#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
dat.sim[[scenario]]=out.ls$sim
dat.sim.sum[[scenario]]=out.ls$sim.sum
out.ls$df$badboot

fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}

plot_fits(dat=dat.use, 
          fits=out.ls$df,
          sim=out.ls$sim)
```


### WC North{.tabset}



```{r}
cur.site="WC_North"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting  data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
out.ls$df$badfit
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```



### WC Bailey{.tabset}



```{r}
cur.site="WC_Bailey"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```




### FR Shore{.tabset}

THis site has a hard time with 1998 - I am removing that here.

```{r}
cur.site="FR_Shore"

## filter data
dat.use.base=dat[dat$Site==cur.site,]
yrs.good=c(1993,1997, 2001, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario,
               yrs.good=yrs.good
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```


### FR Spires{.tabset}


For this site, it becomes apparent that 1993. 1994, and 1998 are problematic, so we remove it here.

```{r}
cur.site="FR_Spires"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

yrs.good=c(
  # 1993,
  # 1994,
  1995,
  1996,
  1997,
  1999,
  2000,
  2001, 
  2003, 
  2004,
  2005,
  2006, 
  2007, 
  2008, 
  2009, 
  2010, 
  2011, 
  2012, 
  2013, 
  2014, 
  2015, 
  2016,
  2017,
  2018, 
  2019)

```


#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario,
               yrs.good = yrs.good
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```





### FR Eaton{.tabset}


We get poor behavior in 1998, so I have removed it. Looking at the data points, we have alternating 0 and 2 count observations, creating a zig-zag pattern. Not surprisingly, this is not well approximated by a gaussian. In general, this site has a number of years that are not particularly gaussian. It looks like the peaks are depressed in many years (2015, 2016, 2014, 1996), lead to plateaus instead of clear peaks. I have still included all of these years but 1998

```{r}
cur.site="FR_Eaton"

## filter data
dat.use.base=dat[dat$Site==cur.site,]
yrs.good=c(1993, 1994, 1995, 1996, 1997, 1999, 2000, 2001, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2011, 2012, 2019, 2014, 2015, 2016, 2017, 2018)

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario,
               yrs.good=yrs.good
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```



### Fir Butte{.tabset}

```{r}
cur.site="Fir_Butte"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)


#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```


### FR GreenOaks{.tabset}


```{r}
cur.site="FR_GreenOaks"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```



### WC FirGrove{.tabset}

Here we have an undefined left half of 2004, so we will cut that year. 2019 is also somewhat ill defined, but I have left it in place for now. There are several other poorly defined years with few data points: 2007, 2008, 2009. I have excluded those here.



```{r}
cur.site="WC_FirGrove"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

yrs.good=c(
  2002,
  2003,
  # 2004,
  3005,
  2006,
  2010,
  2013,
  2014,
  2015,
  2016,
  2017,
  2018,
  2019
)

```



#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)


#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario,
               yrs.good=yrs.good
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```

### WC Hayfield{.tabset}



```{r}
cur.site="WC_Hayfield"

## filter data
dat.use.base=dat[dat$Site==cur.site,]

```


#### yearly plots

```{r}
dat.use=dat.use.base
par(mfrow=c(2,2))
for(cur.year in unique(dat.use$Year)){
  dat.plot=dat.use[dat.use$Year==cur.year,]
  plot(dat.plot$doy, dat.plot$Est_Count, 
       main=cur.year)
}

table(dat.use$Year,dat.use$Est_Count>0)
```

#### fitting data

```{r}
yrs.good=unique(dat.use$Year)
good.df=rbind(good.df, 
              data.frame(site=rep(cur.site, length(yrs.good)),
                         year=yrs.good)
)

#adding zeros
#storage of data through the zero-adding process
## first add zeros on day 1, 365
scenario=paste(cur.site, "base")
out.ls=dat_fit(dat=dat.use.base,
               doymean=mean(dat.use.base$doy),
               doysd=sd(dat.use.base$doy),
               scenario=scenario
)
res.sum=rbind(res.sum, out.ls$summary)
res.df=rbind(res.df, out.ls$df)
fits.indiv[[cur.site]]=out.ls$df[,c("year","b0","b1","b2")]
```

```{r}
plot_fits(dat=dat.use, 
          fits=out.ls$df)
```


## Fitting all at once {.tabset}

The above methods are good for identifying problmeatic years of data. Here, wefit all data at once, which allows more correct capturing of site and year effects (informing data-poor years), and allows for summary statistics of model structures. The general process is:

- use only those sites and years that worked in the various site-level analyses, so that we know our model won't crash, and so that results are most comparable
- fit models cumulatively with (a) fixed effect of intercept, linear and quadratic terms of DOY (basic gaussian linearization approach) (b) Random effect of linear and quadratic DOY terms per year, (c) Random effect of linear and quadratic DOY terms per site, and (d) random effect of lienar and quadratic DOY terms per site-year
- compare model fits to make sure most complex model is the best (it is)
- compare final model coefficients with those from the site-level model-fitting done previously.

After fitting the models, we have summaries of the additional data available from using a heirarchical model, and AIC comparison of models with varying levels of random effects.

Following that we have a section for visual comparisons. Fitted year curve plots have the site-level model fit (solid black) and the full model fit (dashed purple). 



### Analysis


```{r}
## save results so we can examine it later

fits.indiv.old=fits.indiv
good.df$site=as.character(good.df$site)
names(good.df)=c("Site","Year")

# We do not want Hayfield, which is problematic for fitting
good.df=good.df[good.df$Site != "WC_Hayfield",]
dat.good=dat[apply(dat[,c("Site","Year")], 1, paste0, collapse="") %in% 
               apply(good.df, 1, paste0, collapse=""),]
dat.good=cbind(dat.good, doy.sc=scale(dat.good$doy, center=T, scale=T))

## save results so EC can play with it. 
saveRDS(dat, file=here("2_data_wrangling", "dat.rds"))
saveRDS(dat.good, file=here("2_data_wrangling", "dat_good.rds"))
```

#### Comparing sample sizes with idiosyncratic approach

Before we go further, how many sites or years would we have to throw out if we didn't use a heirarchical approach. We start by assume that without random effects for site and year, we would need 6 data points per site (2 data points per parameter fit), and least 3 of those would need to be non-zero.

```{r}

head(dat.good)

dat.min=dat.good %>%
  mutate(siteyear=paste0(Site,Year)) %>%
  rename(count=Est_Count,
         year=Year,
         site=Site)
dat.min=dat.min[,c("count","site","year","siteyear")]

tot.tab=table(dat.min$siteyear)>5
nonzero.tab=table(dat.min$siteyear[dat.min$count>0])>2
usable=tot.tab & nonzero.tab
```

We would have to throw out `r sum(!usable)` site-years, or `r mean(!usable)*100`% of our data. 


```{r}

dat.usable=data.frame(usable=usable)

dat.usable$site=substr(rownames(dat.usable), 1, nchar(rownames(dat.usable))-4)
dat.usable$year=substr(rownames(dat.usable), nchar(rownames(dat.usable))-3, nchar(rownames(dat.usable)))

table(dat.usable$site, dat.usable$usable)

dat.usable %>%
  kable() %>%
  kable_styling()

```




#### Actual analysis
Now for the new analysis.

```{r}
out.n=glm(Est_Count ~ doy.sc + I(doy.sc^2), family = poisson, data = dat.good)
out.y=glmer(Est_Count ~ doy.sc + I(doy.sc^2) + 
              (doy.sc + I(doy.sc^2) | Year), family = poisson, data = dat.good,
            control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
out.s=glmer(Est_Count ~ doy.sc + I(doy.sc^2) + 
              (doy.sc + I(doy.sc^2) | Site), family = poisson, data = dat.good,
            control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
out.ys=glmer(Est_Count ~ doy.sc + I(doy.sc^2) + 
               (doy.sc + I(doy.sc^2) | Year) + 
               (doy.sc + I(doy.sc^2) | Site), family = poisson, data = dat.good,
             control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))
out.ysi=glmer(Est_Count ~ doy.sc + I(doy.sc^2) + 
                (doy.sc + I(doy.sc^2) | Year) + 
                (doy.sc + I(doy.sc^2) | Site)+
                (doy.sc + I(doy.sc^2) | Year : Site), family = poisson, data = dat.good,
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

## Now we can try doing siteyear "by hand"
dat.good$SiteYear=sprintf("%sx%s",dat.good$Site, dat.good$Year)
out.ysi2=out.ysi=glmer(Est_Count ~ doy.sc + I(doy.sc^2) + 
                         (doy.sc + I(doy.sc^2) | Year) + 
                         (doy.sc + I(doy.sc^2) | Site)+
                         (doy.sc + I(doy.sc^2) | SiteYear), family = poisson, data = dat.good,
                       control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

# compare the 2 ways that should fit the same.
# they do
AIC(out.ysi, out.ysi2)

# compare the different models
AIC(out.n, out.s, out.y, out.ys, out.ysi)
require(MuMIn)
library(r2glmm)
r2beta(out.n)$Rsq[1]
r2beta(out.y)$Rsq[1]
r2beta(out.s)$Rsq[1]
r2beta(out.ys)$Rsq[1]
r2beta(out.ysi)$Rsq[1]

```
Let's present that in pretty form.

```{r}
require(knitr)
require(kableExtra)
dat.present = data.frame(model=c("null",
                                 "site only", 
                                 "year only", 
                                 "site and year", 
                                 "site and year and interaction"),
                         AIC(out.n, out.s, out.y, out.ys, out.ysi)
)

kable(dat.present) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```



Now we obtain predictions. Note that this gets somewhat complicated with multiple random effects: the approach that appears correct is to snag the fixed effects with `fixef()`, and the conditional random effects with `ranef()`.

```{r}
out.full=out.ysi #define whichever model we're trying to fit

# object to save coefficient estimates in.
full.ls=list()

#grab random effects
full.ranef=ranef(out.full)
#grab full effects
full.fixed=fixef(out.full)


for(cur.site in unique(dat.good$Site)){
  ## set up storage for bootstrapping

    ## get site RE term
  site.cur=full.ranef$Site[rownames(full.ranef$Site)==cur.site,]
  
  ## get year RE terms
  year.cur=full.ranef$Year
  
  ## get site-year RE terms
  SiteYear.df=full.ranef$SiteYear
  SiteYear.df$Site=gsub("x.*","",rownames(SiteYear.df))
  SiteYear.df$Year=gsub(".*x","",rownames(SiteYear.df))
  siteyear.cur=SiteYear.df[SiteYear.df$Site==cur.site,]
  
  
  # combine coefficients in the correct way
  coefs.cur=siteyear.cur[,1:3] + #site-year RE terms
    year.cur[rownames(year.cur) %in% siteyear.cur$Year,] + # year RE terms
    site.cur[rep(1,nrow(siteyear.cur)),] + #site RE term
    matrix(full.fixed, nrow=1)[rep(1,nrow(siteyear.cur)),] #model fixed effects
  coefs.cur.years=gsub(".*x","",rownames(coefs.cur))
  #unscale coefficients
  coefs.unsc=coefs_unscaled(as.matrix(coefs.cur),
                            mean=mean(dat.good$doy),
                            sd=sd(dat.good$doy))
  coefs.unsc = as.data.frame(coefs.unsc)
  coefs.unsc=cbind(year=gsub(".*x","",rownames(coefs.unsc)),
                   coefs.unsc)
  rownames(coefs.unsc)=NULL
  colnames(coefs.unsc)=c("year","b0","b1","b2")
  
  #calculate phenology metrics
  cur.pheno=data.frame(mu=numeric(nrow(coefs.unsc)),
                 sd=numeric(nrow(coefs.unsc)),
                 fp=numeric(nrow(coefs.unsc)),
                 ofs=numeric(nrow(coefs.unsc)),
                 fst=numeric(nrow(coefs.unsc)),
                 N=numeric(nrow(coefs.unsc)),
                 h=numeric(nrow(coefs.unsc)),
                 badfit = (coefs.unsc$b1 < 0) | (coefs.unsc$b2>0)
  )
  rownames(cur.pheno)=coefs.unsc$year
  cur.pheno$mu=coefs.unsc$b1/(-2*coefs.unsc$b2)
  cur.pheno$sd=sqrt(1/(-2*coefs.unsc$b2))
  cur.pheno$fp=qnorm(.9, mean=cur.pheno$mu, sd=cur.pheno$sd)-
    qnorm(.1, mean=cur.pheno$mu, sd=cur.pheno$sd)
  cur.pheno$ofs=-sqrt(coefs.unsc$b1^2 - 4 * coefs.unsc$b0 * coefs.unsc$b2)/coefs.unsc$b2
  cur.pheno$ofs[is.na(cur.pheno$ofs)]=0
  cur.pheno$fst=qnorm(.1, mean=cur.pheno$mu, sd=cur.pheno$sd)
  cur.pheno$N=sqrt(pi)/sqrt(-coefs.unsc$b2)*exp(coefs.unsc$b0+coefs.unsc$b1^2/(-4*coefs.unsc$b2))
  cur.pheno$h=exp(coefs.unsc$b0 + coefs.unsc$b1*cur.pheno$mu + coefs.unsc$b2*cur.pheno$mu^2)
  
  
  res=data.frame(site=rep(cur.site,nrow(coefs.unsc)),
                 year=coefs.unsc$year,
                 b0=coefs.unsc$b0,
                 b1=coefs.unsc$b1,
                 b2=coefs.unsc$b2,
                 mu=cur.pheno$mu, #day of peak activity
                 fp=cur.pheno$fp, #flight period
                 ofs=cur.pheno$ofs, #flight period
                 N=cur.pheno$N, #abundance index
                 tenperc=cur.pheno$fst, #day of 10% observation
                 h=cur.pheno$N, #day of 10% observation
                 badfit=cur.pheno$badfit #was the best-fitting model implausible?
                 
  )
  
 
  yrs=as.numeric(res$year)
 
  ## store changes in metrics through time
  slope.sum=data.frame(mu=NA,
                       fp=NA,
                       ofs=NA,
                       logN=NA
  )
  # if there are no bad fits
  if(!any(res$badfit)){
    out.mu=lm(cur.pheno$mu ~ yrs)
    out.fp=lm(cur.pheno$fp ~ yrs)
    out.ofs=lm(cur.pheno$ofs ~ yrs)
    out.logN=lm(log(cur.pheno$N) ~ yrs)
    
    slope.sum$mu=coefficients(out.mu)[2]
    slope.sum$fp=coefficients(out.fp)[2]
    slope.sum$ofs=coefficients(out.ofs)[2]
    slope.sum$logN=coefficients(out.logN)[2]
  }
  ## store mean vals across all years, with 95% CI
  vals.sum=data.frame(mu=mean(cur.pheno$mu),
                      fp=mean(cur.pheno$fp),
                      ofs=mean(cur.pheno$ofs),
                      N=mean(cur.pheno$N)
  )
  # add summary info to res.sum
  names(slope.sum)=sprintf("%s.%s", "slope", names(slope.sum))
  
  #save site-level means and slope sims
  full.ls[[cur.site]]=list(df=res,
              summary=data.frame(
                scenario=cur.site,
                vals.sum,
                slope.sum
              ))
}
```


### (Visually) comparing fits{.tabset}

#### WC Main

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="WC_Main"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### WC North

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="WC_North"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### WC Bailey

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="WC_Bailey"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```


#### FR Shore

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="FR_Shore"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### FR Spires

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="FR_Spires"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```


#### FR Eaton

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="FR_Eaton"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### Fir Butte

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="Fir_Butte"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### FR GreenOaks

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="FR_GreenOaks"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```

#### FirGrove

```{r cache=FALSE, fig.width=12, fig.height=10}
cur.site="WC_FirGrove"
cur.indiv=fits.indiv[[cur.site]]
cur.full=full.ls[[cur.site]]$df
x.plot=seq(100,200,by=.1)
par(mfrow=c(2,2))
for(cur.year in cur.indiv$year){
  y.indiv=exp(cur.indiv$b0[cur.indiv$year==cur.year] + 
                cur.indiv$b1[cur.indiv$year==cur.year]*x.plot + 
                cur.indiv$b2[cur.indiv$year==cur.year]*x.plot^2)
  y.full=exp(cur.full$b0[cur.full$year==cur.year] + 
               cur.full$b1[cur.full$year==cur.year]*x.plot + 
               cur.full$b2[cur.full$year==cur.year]*x.plot^2)
  dat.cur=dat.good[dat.good$Year==cur.year & dat.good$Site==cur.site,]
  plot(dat.cur$DOY, dat.cur$Est_Count,
       ylim=c(0, max(dat.cur$Est_Count, y.indiv, y.full)),
       main=paste(cur.site, cur.year),
       xlim=c(110,190)
  )
  points(x.plot, y.indiv, type='l', col='black')
  points(x.plot, y.full, type='l', col='purple', lty=2)
}

```




## Saving Results

```{r, cache=FALSE}

## make metadata file and save results

cat("Details on the various data files generated from script analysis-final.Rmd", 
    file=here("4_res", "README-datafiles.txt"), 
    sep="\n", 
    append=FALSE)

cat(c(as.character(Sys.time()),"",""), 
    file=here("4_res", "README-datafiles.txt"), 
    sep="\n", 
    append=TRUE)

write.csv(res.df, here("4_res","yearly-summary.csv"), row.names = FALSE)
cat("yearly-summary.csv - Similar to sitely-summary.csv, but one row per year per site.This may be more useful for making more detailed plots, including yearly gaussian curves. Because we have individual fitted curves for each of the years, we have included the coefficients of the fitted gaussians as b0, b1, and b2 (unscaling has already been taking care of). Here we have the phenological metrics for each site-year rather than their means or slopes across years. We also include two other metrics, because why not. Tenperc is the day of the 10% quantile, somewhat analogous to the day of first activity. h is the peak activity, which is sometimes used in other papers. The badfit column will be FALSE *unless* the best-fitting values are biologically implausible (which should never happen, as we should have removed those years from our analyses). badboot shows the fraction of initial bootstrapped values that are biologically implausible for that site-year. bootnum shows the number of parametric bootstrapped simulations that were actually used after rejecting biologically implausible ones (this should always be the same number, as we generate extras in case we must reject some).\n", 
    file=here("4_res", "README-datafiles.txt"), 
    sep="\n", 
    append=TRUE)
```


## Extracting and saving full analysis results

```{r, cache=FALSE}
full.res=NULL
full.summary=NULL
for(i in 1:length(full.ls)){
 full.res=rbind(full.res, full.ls[[i]]$df)
 full.summary=rbind(full.summary, full.ls[[i]]$summary)
}

write.csv(full.summary, here("4_res","sitely-summary-full.csv"), row.names = FALSE)
cat("sitely-summary-full.csv - as sitely-summary.csv, but fit using a single model across all sites. main results of our baseline analysis, and likely to be the basis of most figures. Each row represents a single site. The first set of columns represent either the averages (\"vals.*\") across years or slopes (\"slopes.*\") through years of key phenological metrics (mu = day of peak abundance; fp = days from 0.1 to 0.9 quantiles of gaus furve; N = abundance index) along with 95% CI as measured by 0.025 and 0.975 quantiles of parametric bootstrapping. The final two columns show the fraction of bootstrapped values that were biologically implausible (these were removed before metrics were calculated) across all years and the highest such fraction among the years. Note the new inclusion of the Observable Flight Seasion, denoted ofs.\n", 
    file=here("4_res", "README-datafiles.txt"), 
    sep="\n", 
    append=TRUE)

write.csv(full.res, here("4_res","yearly-summary-full.csv"), row.names = FALSE)
cat("yearly-summary-full.csv - Similar to sitely-summary-full.csv, but one row per year per site.This may be more useful for making more detailed plots, including yearly gaussian curves. Because we have individual fitted curves for each of the years, we have included the coefficients of the fitted gaussians as b0, b1, and b2 (unscaling has already been taking care of). Here we have the phenological metrics for each site-year rather than their means or slopes across years. We also include two other metrics, because why not. Tenperc is the day of the 10% quantile, somewhat analogous to the day of first activity. h is the peak activity, which is sometimes used in other papers. The badfit column will be FALSE *unless* the best-fitting values are biologically implausible (which should never happen, as we should have removed those years from our analyses). badboot shows the fraction of initial bootstrapped values that are biologically implausible for that site-year. bootnum shows the number of parametric bootstrapped simulations that were actually used after rejecting biologically implausible ones (this should always be the same number, as we generate extras in case we must reject some).\n", 
    file=here("4_res", "README-datafiles.txt"), 
    sep="\n", 
    append=TRUE)

```


